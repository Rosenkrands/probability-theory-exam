% !TEX root = ../main.tex
\subsection{Simulation of Continuous Distributions}
\begin{boks}{Proposition 5.2 (The Inverse Transformation Method)}
  Let $F$ be a distribution function that is continuous and strictly increasing. Further, let $U \sim \unif[0,1]$ and define the random variable $Y = F^{-1}(U)$. Then Y has distribution function $F$.
\end{boks}
\begin{proof}
  Start with $F_Y$, the distribution function of $Y$. Take $x$ in the range of $Y$ to obtain
  \begin{align*}
    F_Y(x) &= P(F^{-1}(U) \leq x) \\
    &= P(U \leq F(x)) = F_U(F(x)) = F(x)
  \end{align*}
  where the last equality follows since $F_U(u) = u$ if $0 \leq u \leq 1$. The argument here is $u = F(x)$, which is between $0$ and $1$ since $F$ is a cdf.
\end{proof}

\begin{boks}{Example 5.2}
  Generate an observation from an exponential distribution with parameter $\lambda$.

  Here,
  \begin{align*}
    F(x) = 1- e^{-\lambda x}, \quad x \geq 0
  \end{align*}
  To find the inverse, as usual solve $F(x) = u$, to obtain
  \begin{align*}
    x = F^{-1}(u) = -\frac{1}{\lambda} \log(1 - u), \quad 0 \leq u < 1
  \end{align*}
  Hence if $U \sim \unif[0,1]$, the random variable
  \begin{align*}
  X = -\frac{1}{\lambda} \log(1 - U)
  \end{align*}
  is exp($\lambda$). We can note here that since $1 - U$ is also uniform on $[0,1]$, we might as well take $X = - \log U/\lambda$.
\end{boks}

\begin{boks}{Proposition 5.3 (The Rejection Method)}
  \begin{enumerate}[label = \arabic*.]
    \item Generate $Y$ and $U \sim \unif[0,1]$ independent of each other.
    \item If $U \leq \frac{f(Y)}{cg(Y)}$, set $X = Y$. Otherwise return to step $1$.
  \end{enumerate}
  The random variable $X$ generated by the algorithm has pdf $f$.
\end{boks}
\begin{proof}
  Let us first make sure that the algorithm terminates. The probability in any given step 2 to accept $Y$ is, by Corollary 3.2,
  \begin{align*}
    P((X, Y) \in B) = \int_{\mathbb{R}} P((x, Y) \in B)f_X(x) dx
  \end{align*}
  where
  \begin{align*}
    (Y, U) \quad \text{and} \quad B = \left\{(Y, U) \ : \ U \leq \frac{f(y)}{cg(y)}\right\}
  \end{align*}
  \begin{align*}
    P\left( U \leq \frac{f(y)}{cg(y)} \right) &=
    \int_{\mathbb{R}} P\left( U \leq \frac{f(y)}{cg(y)} \right) g(y)dy \\
    &= \int_{\mathbb{R}} \frac{f(y)}{cg(y)} g(y)dy =
    \frac{1}{c}\int_\mathbb{R}f(y)dy =
    \frac{1}{c}
  \end{align*}
  where we used the independence of $U$ and $Y$ and the fact that $U \sim \unif[0,1]$. Hence, the number of iterations until we accept a value has a geometric distribution with success probability $1/c$. The algorithm therefore always terminates in a number of steps with mean $c$ from which it also follows that we should choose $c$ as small as possible.

  Next we turn to the question of why this gives the corresct distribution. To show this, we will show that the conditional distribution of $Y$, given acceptance, is the same as the distribution of $X$. Recalling the definition of conditional probability and the fact that the probability of acceptance is $1/c$, we get from
  \begin{align*}
    P(A|B) = \frac{P(A \cap B)}{P(B)}
  \end{align*}
  that
  \begin{align*}
    P\left(Y \leq x \ \bigg| \ U \leq \frac{f(y)}{cg(y)} \right) =
    \frac{P\left(Y \leq x \ \cap \ U \leq \frac{f(y)}{cg(y)}\right)}{1/c} = cP\left(Y \leq x \ \cap \ U \leq \frac{f(y)}{cg(y)}\right)
  \end{align*}
  By independence, the joint pdf of $(U, Y)$ is $f(u,y) = g(y)$, and the above expression becomes
  \begin{align*}
    cP\left(Y \leq x \ \cap \ U \leq \frac{f(y)}{cg(y)}\right) &= c \int_{-\infty}^x \int_0^{f(y)/cg(y)} g(y)du \ dy \\
    &= c \int_{-\infty}^x \frac{f(y)}{cg(y)}g(y)dy = P(X \leq x)
  \end{align*}
  which is what we wanted to prove.
\end{proof}
