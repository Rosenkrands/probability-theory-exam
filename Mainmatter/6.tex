% !TEX root = ../main.tex

% \textbf{Noget af det der er med her skal postuleres uden bevis!}

% \textit{Generating functions}, or \textit{transforms}, are very useful in probability theory as in other fields of mathematics. Several different generating functions are used, depending on the type of random variable. We will discuss two, one that is useful for discrete random variables and the other for continuous random variables.

% \subsection{The Probability Generating Function}
% When we study nonnegative, integer-valued random variables, the following function proves to be a useful tool.
%
\begin{boks}{Definition 3.23}
  Let $X$ be nonnegative and integer valued. The function
  \begin{align*}
    G_X(s) = E[s^X] = \sum_{k=0}^\infty s^kP(X=k) \quad 0 \leq s \leq 1
  \end{align*}
  is called the \textit{probability generating function} (pgf) of $X$.
\end{boks}
Note that by Proposition 2.12 we compute $G_X$ as the power series
\begin{align*}
  G_X(s) = \sum_{k = 0}^\infty s^k P_X(k), \quad 0 \leq s \leq 1
\end{align*}
where $p_X$ is the pmf of $X$. If the range of $X$ is finite, the sum is finite. Note that
\begin{align*}
  G_X(s) = p_X(0) + \sum_{k = 1}^\infty s^k p_X(k)
\end{align*}
which immediately gives the following corrolarry
\begin{boks}{Corollary 3.12}
  Let $X$ be a nonnegative and integer valued with pgf $G_X$. then
  \begin{align*}
    G_X(0) = p_X(0) \quad \text{and} \quad G_X(1) = 1
  \end{align*}
\end{boks}
%
\begin{boks}{Proposition 3.36}
  Let $X$ be nonnegative and integer valued with pgf $G_X$. then
  \begin{align*}
    p_X(k) = \frac{G_X^{(k)}(0)}{k!}, \quad k = 0,1,\ldots
  \end{align*}
  where $G_X^{(k)}(0)$ denotes the $k$th derivative of $G_X$.
\end{boks}
%
% \begin{boks}{Proposition 3.37}
%   If $X$ has pgf $G_X$, Then
%   \begin{align*}
%     E[X] = G'_X(1) \quad \text{and} \quad Var[X] = G''_X(1) + G'_X(1) - G'_X(1)^2
%   \end{align*}
% \end{boks}
%
\begin{boks}{Proposition 3.38}
  Let $X_1, X_2, \ldots, X_n$ be independent random variables with pgfs $G_1, G_2, \ldots, G_n$ respectively and let $S_n = X_1 + X_2 + \cdots + X_n$. Then $S_n$ has pgf
  \begin{align*}
    G_{S_n}(s) = G_1(s)G_2(s)\cdots G_n(s), \quad 0 \leq s \leq 1
  \end{align*}
\end{boks}

\begin{proof}
  Since $X_1, \ldots, X_n$ are independent, the random variables $s^{X_1}, \ldots, s^{X_n}$ are also independent for each $s$ in [0,1], and we get
  \begin{align*}
    G_{S_n}(s) &= E[s^{X_1 + X_2 + \cdots + X_n}]\\
    &= E[s^{X_1}]E[s^{X_2}] \cdots E[s^{X_n}]\\
    &= G_1(s)G_2(s) \cdots G_n(s)
  \end{align*}
  and we are done.
\end{proof}
%
\begin{boks}{Proposition 3.39}
  Let $X_1, X_2, \ldots$ be i.i.d. nonnegative and integer valued with commen pgf $G_X$, and let $N$ be nonnegative and integer valued, and independent of the $X_k$, with pgf $G_N$. Then $S_N = X_1 + \cdots + X_N$ has pgf
  \begin{align*}
    G_{S_N}(s) = G_N(G_X(s))
  \end{align*}
  the composition of $G_N$ and $G_X$.
\end{boks}

\begin{proof}
  We condition on $N$ to obtain
  \begin{align*}
    G_{S_N}(s) &= E\left[ E\left[ s^{S_N} | N = n \right] \right] \\
    &= \sum_{n = 0}^\infty E[s^{S_N} | N = n] P(N = n) \\
    %Muligt at fjerne betingelsen som følge af uafhængighed
    &= \sum_{n = 0}^\infty E[s^{S_N}] P(N = n)
  \end{align*}
  since $N$ and $S_n$ are independent. Now note that
  \begin{align*}
    E[s^{S_N}] = G_X(s)^n
  \end{align*}
  by Proposition 3.38 and we get
  \begin{align*}
    G_{S_N}(s) = \sum_{n = 0}^\infty G_X(s)^n P(N = n) = G_N(G_X(s))
  \end{align*}
  the pgf of $N$ evaluated at the point $G_X(s)$.
\end{proof}
%
% \begin{boks}{Corollary 3.13}
%   Under the assumptions of Proposition 3.39, it holds that
%   \begin{align*}
%     E[S_N] &= E[N]\mu \\
%     Var[S_N] &= E[N]\sigma^2 + Var[N]\mu^2
%   \end{align*}
%   where $\mu = E[X_k]$ and $\sigma^2 = Var[X_k]$.
% \end{boks}
%
% \subsection{The Moment Generating Function}
% The probability generating function is an excellent tool for nonnegative and integer-valued random variables.
% For other random variables, we can instead use the following more general generating function.
%
% \begin{boks}{Definition 3.24}
%   Let $X$ be a random variable. The function
%   \begin{align*}
%     M_X(t) = E[e^{tX}], \quad t \in \mathbb{R}
%   \end{align*}
%   is calle the \textit{moment generating function} (mgf) of $X$.
% \end{boks}
%
% If $X$ is continuous with pdf $f_X$, we compute the mgf by
% \begin{align*}
%   M_X(t) = \int_{-\infty}^\infty e^{tx} f_X(x)dx
% \end{align*}
% and for discrete $X$, we get a sum instead. Note that if $X$ is nonnegative integer valued with pgf $G_X$, then
% \begin{align*}
%   M_X(t) = G_X(e^t)
% \end{align*}
% which immediately give the mgf for the distributions for which we computed the pgf above.
%
% \begin{boks}{Corollary 3.15}
%   If $X$ has mgf $M_X(t)$, then
%   \begin{align*}
%     E[X] = M'_X(0) \quad \text{and} \quad Var[X] = M''_X(0) - (M'_X(0))^2
%   \end{align*}
% \end{boks}
%
% \begin{proof}
%   By differentiating $M_X(t)$ with respect to $t$, we get
%   \begin{align*}
%     M'_X(t) = \frac{d}{dt}E[e^{tx}] = E[Xe^{tx}]
%   \end{align*}
%   where we assumed that we can interchange differentiation and expectation.
%   Note how $t$ is the variable of differentiation and we view $X$ as fixed.
%   In the case of a discrete $X$, this amounts to differentiating a sum termwise, and in the case of a continuous $X$, it means that we can differentiate under the integral sign.
%   This is by no means obvious but can be verified.
%   We will not address this issue further.
%   Differentiating once more gives
%   \begin{align*}
%     M''_X(t) = \frac{d}{dt} E[Xe^{tx}] = E[X^2e^{tx}]
%   \end{align*}
%   which gives
%   \begin{align*}
%     Var[X] = E[X^2] - (E[X])^2 = M''_X(0) - (M'_X(0))^2
%   \end{align*}
%   as desired.
% \end{proof}
%
% Note we get the general result
% \begin{align*}
%   E[X^n] = M^{(n)}_X(0), \quad n = 1, 2, \ldots
% \end{align*}
% where $M_X^{(n)}$ is the $n$th derivative of $M_X$.
% The number $E[X^n]$ is called the $n$th \textit{moment} of $X$, hence the term moment generating function. Compare it with the probability generating function that generates probabilities by differentiating and setting $s = 0$. The moment generating function also turns sum into products, according to the following proposition, which you may prove as an excercise.
% \begin{boks}{Proposition 3.40}
%   Let $X_1, X_2, \ldots, X_n$ be independent random variables with mgfs $M_1, M_2, \ldots, M_n$, respectively, and let $S_n = X_1 + \cdots + X_n$. Then $S_n$ has mgfs
%   \begin{align*}
%     M_{S_n}(t) = M_1(t)M_2(t)\cdots M_n(t), \quad t \in \mathbb{R}
%   \end{align*}
% \end{boks}

% \subsection{The Poisson Proces}
% \begin{boks}{Definition 3.25}
%   A point process where times between consecutive points are i.i.d. random variables that are exp($\lambda$) is called a $Poisson$ $process$ with rate $\lambda$.
% \end{boks}
%
% \begin{boks}{Proposition 3.41}
%   Consider a Poisson process with rate $\lambda$, where $X(t)$ is the number of points in an interval of length $t$.
%   Then
%   \begin{align*}
%     X(t) \sim \text{Poi}(\lambda t)
%   \end{align*}
% \end{boks}
% Recall that the parameter in the Poisson distribution is also the expected value.
% Hence, we have
% \begin{align*}
%   E[X(t)] = \lambda t
% \end{align*}
% which makes sense since $\lambda$ is the mean number of points per time unit and $t$ is the length of the time interval.
% In practical applications, we need to be careful to use the same time units for $\lambda$ and $t$.
% \begin{boks}{Proposition 3.42}
%   In a Poisson process with rate $\lambda$
%   \begin{enumerate}
%     \item $T_1, T_2, \ldots$ are independent and exp($\lambda$)
%     \item $X(T_1), X(T_2), \ldots$ are independent and $X(t_j) \sim Poi(\lambda t_j), \quad j = 1, 2, \ldots$
%   \end{enumerate}
% \end{boks}

% \begin{boks}{Proposition 3.43}
%   Consider a Poisson process with rate $\lambda$. If there are $n$ points in the time interval $[0,t]$, their joint distribution is the same as that of $n$ i.i.d.\ random variables that are unif$[0,t]$
% \end{boks}
%
% The property in the proposition is called the \emph{order statistic property} of the Poisson process since the points are distributed as $n$ order statistics from a uniform distribution on $[0,t]$.
% The proof is similar to what we did previously in the case of $n = 1$,
% invoking Proposition 3.32.

% \subsection{Thinning and Superposition}
% Suppose now that we have a Poisson process with rate $\lambda$, where we do not observe every point, either by accident or on purpose.
% For example, phone calls arrive as a Poisson process, but calls are lost when the line is busy.
% Hurricanes ate formed according to a Poisson process, but we are interested only in those that make landfall.
% These examples of \emph{thinning} of a Poisson process.
%
% We assume that each point is observed with probability $p$ and that different points are observed independent of each other.
% Then, it turns out that the thinned process is also Poisson process.

% \begin{boks}{Proposition 3.44}
%   The thinned process is a Poisson process with rate $\lambda p$.
% \end{boks}
%
% \begin{proof}
%   We work with characterization (b) in Proposition 3.42. Clearly, the numbers of observed points in disjoint intervals are independent.
%   To show the Poisson distribution, we use probability generating functions.
%   Consider an interval of length $t$, letting $X(t)$ be the total number of points and $X_p(t)$ be the number of observed points in this interval.
%   Then,
%   \begin{align*}
%     X_p(t) = \sum_{k = 1}^{X(t)} I_k
%   \end{align*}
%   where $I_k$ is 1 if the $k$th point was observed and 0 otherwise. By Proposition 3.39, $X_p(t)$ has pgf
%   \begin{align*}
%     G_{X_p}(s) = G_{X(t)}(G_{I}(s))
%   \end{align*}
%   where
%   \begin{align*}
%     G_{X(t)}(s) = e^{\lambda t (s - 1)}
%   \end{align*}
%   and
%   \begin{align*}
%     G_I(s) = 1 - p + ps
%   \end{align*}
%   and we get
%   \begin{align*}
%     G_{X_p}(s) = e^{\lambda t(1 - p + ps - 1)} = e^{\lambda pt(s - 1)}
%   \end{align*}
%   which we recognize as the pgf of a Poisson distribution with parameter $\lambda pt$.
% \end{proof}

% \begin{boks}{Proposition 3.45}
%   The processes of observed and unobserved points are independent.
% \end{boks}
%
% \begin{proof}
%   Fix an interval of length $t$, let $X(t)$ be the total number of points, and $X_p(t)$ and $X_{1 - p}(t)$ the number of observed and unobserved points, respectively.
%   Hence, $X(t) = X_p(t) + X_{1 - p}(t)$ and by Proposition 3.44, we obtain
%   \begin{align*}
%     X_p(t) \sim \poi(\lambda pt) \quad \text{and} \quad X_{1 - p}(t) \sim \poi(\lambda(1 - p)t).
%   \end{align*}
%   Also given that $X(t) = n$, the number of observed points has a binomial distribution with parameters $n$ and $p$. We get
%   \begin{align*}
%     P(X_p(t) = j, X_{1-p}(t) = k) &= P(X_p(t) = j, X(t) = k + j)\\
%     &= P(X_p(t) = j | X(t) = k + j)P(X(t) = k + j)\\
%     &=\binom{k + j}{j} p^j(1 - p)^ke^{-\lambda t} \frac{(\lambda t)^{k + j}}{(k + j)!}\\
%     &= \frac{(k + j)!}{k!j!}p^j(1 - p)^ke^{-\lambda t} \frac{(\lambda t)^{k + j}}{(k + j)!}\\
%     &= e^{-\lambda pt} \frac{(\lambda pt)^j}{j!}e^{-\lambda(1 - p)t} \frac{(\lambda(1 - p)t)^k}{k!}\\
%     &= P(X_p(t) = j)P(X_{1 - p}(t) = k)
%   \end{align*}
% \end{proof}

% \begin{boks}{Proposition 3.46}
%   Consider two independent Poisson processes with rate $\lambda_1$ and $\lambda_2$, respectively. The superposition of the two processes is a Poisson process with rate $\lambda_1 + \lambda_2$.
% \end{boks}
%
% \begin{proof}
%   For this we use characterization (a) in Proposition 3.42.
%   Fix a time where there is a point in either of the two processes.
%   Denote the time until the next point in the first process by $S$ and the corresponding time in the second by $T$.
%   Since the two processes are independent, $S$ and $T$ are independent random variables.
%   Also, by the memoryless property, the distributions are $S \sim exp(\lambda_1)$ and $T \sim exp(\lambda_2)$, regardless of which of the two processes the last point came from. Hence, the time until the next point in the superposition process is the minimum of $S$ and $T$, and by example 3.50, this is $exp(\lambda_1 + \lambda_2)$.
%   Thus, the superposition process i s a Poisson process with rate $\lambda_1 + \lambda_2$ and we are done.
% \end{proof}
