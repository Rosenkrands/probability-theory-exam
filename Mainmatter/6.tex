% !TEX root = ../main.tex
\textit{Generating functions}, or \textit{transforms}, are very useful in probability theory as in other fields of mathematics. Several different generating functions are used, depending on the type of random variable. We will discuss two, one that is useful for discrete random variables and the other for continuous random variables.

\subsection{The Probability Generating Function}
When we study nonnegative, integer-valued random variables, the following function proves to be a useful tool.

\begin{boks}{Definition 3.23}
  Let $X$ be nonnegative and integer valued. The function
  \begin{align*}
    G_X(s) = E[s^X] = \sum_{k=0}^\infty s^kP(X=k) \quad 0 \leq s \leq 1
  \end{align*}
  is called the \textit{probability generating function} (pgf) of $X$.
\end{boks}

\begin{boks}{Corollary 3.12}
  Let $X$ be a nonnegative and integer valued with pgf $G_X$. then
  \begin{align*}
    G_X(0) = p_X(0) \quad \text{and} \quad G_X(1) = 1
  \end{align*}
\end{boks}

\begin{boks}{Proposition 3.36}
  Let $X$ be nonnegative and integer valued with pgf $G_X$. then
  \begin{align*}
    p_X(k) = \frac{G_X^{(k)}(0)}{k!}, \quad k = 0,1,\ldots
  \end{align*}
  where $G_X^{(k)}(0)$ denotes the $k$th derivative of $G_X$.
\end{boks}

\begin{boks}{Proposition 3.37}
  If $X$ has pgf $G_X$, Then
  \begin{align*}
    E[X] = G'_X(1) \quad \text{and} \quad Var[X] = G''_X(1) + G'_X(1) - G'_X(1)^2
  \end{align*}
\end{boks}

\begin{boks}{Proposition 3.38}
  Let $X_1, X_2, \ldots, X_n$ be independent random variables with pgfs $G_1, G_2, \ldots, G_n$ respectively and let $S_n = X_1 + X_2 + \cdots + X_n$. Then $S_n$ has pgf
  \begin{align*}
    G_{S_n}(s) = G_1(s)G_2(s)\cdots G_n(s), \quad 0 \leq s \leq 1
  \end{align*}
\end{boks}

\begin{proof}
  Since $X_1, \ldots, X_n$ are independent, the random variables $s^{X_1}, \ldots, s^{X_n}$ are also independent for each $s$ in [0,1], and we get
  \begin{align*}
    G_{S_n}(s) &= E[s^{X_1 + X_2 + \cdots + X_n}]\\
    &= E[s^{X_1}]E[s^{X_2}] \cdots E[s^{X_n}]\\
    &= G_1(s)G_2(s) \cdots G_n(s)
  \end{align*}
  and we are done.
\end{proof}

\begin{boks}{Proposition 3.39}
  Let $X_1, X_2, \ldots$ be i.i.d. nonnegative and integer valued with commen pgf $G_X$, and let $N$ be nonnegative and integer valued, and independent of the $X_k$, with pgf $G_N$. Then $S_N = X_1 + \cdots + X_N$ has pgf
  \begin{align*}
    G_{S_N}(s) = G_N(G_X(s))
  \end{align*}
  the composition of $G_N$ and $G_X$.
\end{boks}

\begin{proof}
  We condition on $N$ to obtain
  \begin{align*}
    G_{S_N}(s) &= E\left[ E\left[ s^{S_N} | N = n \right] \right] \\
    &= \sum_{n = 0}^\infty E[s^{S_N} | N = n] P(N = n) \\
    %Muligt at fjerne betingelsen som følge af uafhængighed
    &= \sum_{n = 0}^\infty E[s^{S_N}] P(N = n)
  \end{align*}
  since $N$ and $S_n$ are independent. Now note that
  \begin{align*}
    E[s^{S_N}] = G_X(s)^n
  \end{align*}
  by Proposition 3.38 and we get
  \begin{align*}
    G_{S_N}(s) = \sum_{n = 0}^\infty G_X(s)^n P(N = n) = G_N(G_X(s))
  \end{align*}
  the pgf of $N$ evaluated at the point $G_X(s)$.
\end{proof}

\begin{boks}{Corollary 3.13}
  Under the assumptions of Proposition 3.39, it holds that
  \begin{align*}
    E[S_N] &= E[N]\mu \\
    Var[S_N] &= E[N]\sigma^2 + Var[N]\mu^2
  \end{align*}
  where $\mu = E[X_k]$ and $\sigma^2 = Var[X_k]$.
\end{boks}

\subsection{The Moment Generating Function}
The probability generating function is an excellent tool for nonnegative and integer-valued random variables.
For other random variables, we can instead use the following more general generating function.

\begin{boks}{Definition 3.24}
  Let $X$ be a random variable. The function
  \begin{align*}
    M_X(t) = E[e^{tX}], \quad t \in \mathbb{R}
  \end{align*}
  is calle the \textit{moment generating function} (mgf) of $X$.
\end{boks}

If $X$ is continuous with pdf $f_X$, we compute the mgf by
\begin{align*}
  M_X(t) = \int_{-\infty}^\infty e^{tx} f_X(x)dx
\end{align*}
and for discrete $X$, we get a sum instead. Note that if $X$ is nonnegative integer valued with pgf $G_X$, then
\begin{align*}
  M_X(t) = G_X(e^t)
\end{align*}
which immediately give the mgf for the distributions for which we computed the pgf above.

\begin{boks}{Corollary 3.15}
  If $X$ has mgf $M_X(t)$, then
  \begin{align*}
    E[X] = M'_X(0) \quad \text{and} \quad Var[X] = M''_X(0) - (M'_X(0))^2
  \end{align*}
\end{boks}

\begin{proof}
  By differentiating $M_X(t)$ with respect to $t$, we get
  \begin{align*}
    M'_X(t) = \frac{d}{dt}E[e^{tx}] = E[Xe^{tx}]
  \end{align*}
  where we assumed that we can interchange differentiation and expectation.
  Note how $t$ is the variable of differentiation and we view $X$ as fixed.
  In the case of a discrete $X$, this amounts to differentiating a sum termwise, and in the case of a continuous $X$, it means that we can differentiate under the integral sign.
  This is by no means obvious but can be verified.
  We will not address this issue further.
  Differentiating once more gives
  \begin{align*}
    M''_X(t) = \frac{d}{dt} E[Xe^{tx}] = E[X^2e^{tx}]
  \end{align*}
  which gives
  \begin{align*}
    Var[X] = E[X^2] - (E[X])^2 = M''_X(0) - (M'_X(0))^2
  \end{align*}
  as desired.
\end{proof}

Note we get the general result
\begin{align*}
  E[X^n] = M^{(n)}_X(0), \quad n = 1, 2, \ldots
\end{align*}
where $M_X^{(n)}$ is the $n$th derivative of $M_X$.
The number $E[X^n]$ is called the $n$th \textit{moment} of $X$, hence the term moment generating function. Compare it with the probability generating function that generates probabilities by differentiating and setting $s = 0$. The moment generating function also turns sum into products, according to the following proposition, which you may prove as an excercise.
\begin{boks}{Proposition 3.40}
  Let $X_1, X_2, \ldots, X_n$ be independent random variables with mgfs $M_1, M_2, \ldots, M_n$, respectively, and let $S_n = X_1 + \cdots + X_n$. Then $S_n$ has mgfs
  \begin{align*}
    M_{S_n}(t) = M_1(t)M_2(t)\cdots M_n(t), \quad t \in \mathbb{R}
  \end{align*}
\end{boks}

\subsection{The Poisson Proces}
\begin{boks}{Definition 3.25}
  A point process where times between consecutive points are i.i.d. tandom variiables that are exp($\lambda$) is called a $Poisson$ $process$ with rate $\lambda$.
\end{boks}

\begin{boks}{Proposition 3.41}
  Consider a Poisson process with rate $\lambda$, where $X(t)$ is the number of points in an interval of length $t$.
  Then
  \begin{align*}
    X(t) \sim \text{Poi}(\lambda t)
  \end{align*}
\end{boks}
Recall that the parameter in the Poisson distribution is also the expected value.
Hence, we have
\begin{align*}
  E[X(t)] = \lambda t
\end{align*}
which makes sense since $\lambda$ is the mean number of points per time unit and $t$ is the length of the time interval.
In practical applications, we need to be careful to use the same time units for $\lambda$ and $t$.
\begin{boks}{Proposition 3.42}
  In a Poisson process with rate $\lambda$
  \begin{enumerate}
    \item $T_1, T_2, \ldots$ are independent and exp($\lambda$)
    \item $X(T_1), X(T_2), \ldots$ are independent and $X(t_j) ~ Poi(\lambda t_j), \quad j = 1, 2, \ldots$
  \end{enumerate}
\end{boks}

\begin{boks}{Proposition 3.43}
  Consider a Poisson process with rate $\lambda$. If there are $n$ points in the time interval $[0,t]$, their
\end{boks}
