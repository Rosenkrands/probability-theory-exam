% !TEX root = ../main.tex
When we introduced expected values, we argued that these could be considered averages of a large number of observations. Thus, if we have observations $X_1, X_2, \ldots, X_n$ and we do not know the mean $\mu$, a reasonable approximation ought to be the \textit{sample mean}
\begin{align*}
  \bar{X} = \frac{1}{n} \sum_{k = 1}^{n} X_k
\end{align*}
in other words, the average of $X_1, \ldots, X_n$. Suppose now that the $X_k$ are i.i.d. with mean $\mu$ and variance $\sigma^2$. By the formulas for the mean and variance of sums of independent variables, we get
\begin{align*}
  E[\bar{X}] = E\left[ \frac{1}{n} \sum_{k = 1}^{n} X_k \right] = \sum_{k = 1}^{n} \frac{1}{n} E[X_k] = \mu
\end{align*}
and
\begin{align*}
  Var[\bar{X}] = Var \left[\frac{1}{n} \sum_{k = 1}^{n} X_k\right] = \sum_{k = 1}^{n} \frac{1}{n^2} Var[X_k] = \frac{\sigma^2}{n}
\end{align*}
that is, $\bar{X}$ has the same expected value as each individual $X_k$ and a variance that becomes smaller the larger the value of $n$.

\subsection{The Law of Large Numbers}
ALthough we can nevet guarantee that $|\bar{X} - \mu|$ is smaller than a given $\varepsilon$ we can say that is very likely that $|\bar{X} - \mu|$ is small if $n$ is large. That is the idea behind the following result.
\begin{boks}{Theorem 4.1. (The Law of Large Numbers)}
  Let $X_1, X_2, \ldots$ be a sequence of i.i.d. random variables with mean $\mu$, and let $\bar{X}$ be their sample mean. Then, for every $\varepsilon > 0$
  \begin{align*}
    P(|\bar{X} - \mu| > \varepsilon) \rightarrow 0 \quad
    \text{as} \quad n \rightarrow \infty
  \end{align*}
\end{boks}
\begin{proof}
  Assume that the $X_k$ have finite variance, $\sigma^2 < \infty$.
  Apply Chebyshev's inequality to $\bar{X}$ and let $c = \varepsilon \sqrt{n}/\sigma$.
  Since $E[\bar{X}] = \mu$ and $Var[\bar{X}] = \sigma^2/n$, we get
  \begin{align*}
    P(|\bar{X} - \mu| > \varepsilon) \leq \frac{\sigma^2}{n\varepsilon^2} \rightarrow 0 \quad
    \text{as} \quad n \rightarrow \infty
  \end{align*}
The assumptions of finite variance is neccessary for this proof to work. However, the law of large numbers is tru also if the varinace is infinite, but the proof in that case is more involved and we will not give it.
\end{proof}
We say that $\bar{X}$ \textit{converges in probability} to $\mu$ and write
\begin{align*}
  \bar{X} \xrightarrow{P} \mu \quad \text{as} \quad n \rightarrow \infty
\end{align*}
\begin{boks}{Corollary 4.1}
  Consider an experiment where the event $A$ occurs with probability $p$. Repeat the experiment independently, let $S_n$ be the number of times we get the event $A$ in $n$ trials, and let $f_n = S_n/n$, the relative frequency. Then
  \begin{align*}
    f_n \xrightarrow{P} p \quad \text{as} \quad n \rightarrow \infty
  \end{align*}
\end{boks}
\begin{proof}
  Define the indicators
  \begin{align*}
    I_k =
    \begin{cases}
      1 \qquad \text{if we get $A$ in the $k$th trial} \\
      0 \qquad \text{otherwise}
    \end{cases}
  \end{align*}
for $k = 1, 2, \ldots, n$. Then the $I_k$ are i.i.d. and we know from Section 2.5.1 that they have mean $\mu = p$. Since $f_n$ is the sample mean of the $I_k$, the law of large numbers gives $f_n \xrightarrow{P} p$ as $n \rightarrow \infty$.
\end{proof}
\begin{boks}{Theorem 4.2 (The Central Limit Theorem)}
  Let $X_1, X_2, \ldots$ be i.i.d. random variables with mean $\mu$ and variance $\sigma^2 < \infty$ and let $S_n = \sum_{k = 1}^{n} X_k$. Then, for each $x \in \mathbb{R}$, we have
  \begin{align*}
    \sqrt{n}\frac{\bar{X}_n - \mu}{\sigma} \quad \xrightarrow[n \rightarrow \infty]{d} \quad N(0,1)
  \end{align*}
  as $n \rightarrow \infty$, where $\Phi$ is the cdf of the standard normal distribution.
\end{boks}
\begin{boks}{Definition 4.1}
  Let $X_1, X_2, \ldots$ be a sequence of discrete random variables such that $X_n$ has pmf $p_{X_n}$. If $X$ is a discrete random variable with pmf $p_X$ and
  \begin{align*}
    p_{X_n}(x) \rightarrow p_{X}(x) \quad
    \text{as} \quad n \rightarrow \infty \quad \text{for all} \quad x
  \end{align*}
  then we say that $X_n$ \textit{converges in distribution} to $X$, written $X_n \xrightarrow{d} X$.
\end{boks}
\begin{boks}{Proposition 4.2}
  Let $X_1, X_2, \ldots$ be a sequence of random variables such that $X_n \sim bin(n, p_n)$, where $np_n \rightarrow \lambda > 0$ as $n \rightarrow \infty$, ans let $X \sim \poi(\lambda)$. Then $X_n \xrightarrow{d} X$.
\end{boks}

\subsection{Continuous Limits}
Let us next consider the case when the limiting random variable is continuous. As we already know from the de Moivre-Laplace theorem, the limit can be continuous even if the random variables themselves are not.
\begin{boks}{Definition 4.2}
  Let $X_1, X_2, \ldots$ be a sequence of random variables such that $X_n$ has cdf $F_n$. If $X$ is a continuous random variable with cdf $F$ random
  \begin{align*}
    F_n(x) \rightarrow F(x)
    \quad \text{as} \quad
    n \rightarrow \infty
    \quad \text{for all} \quad
    x \in \mathbb{R}
  \end{align*}
we say that $X_n$ \textit{converges in distribution} to $X$, written $X_n \xrightarrow{d} X$.
\end{boks}
The most important result of this type is the central limit theorem. Another class of important results regarding convergence in distribution deals with the so-called \textit{extreme values}, for example, the minimum and maximum in a sequence of random variables.
